# ğŸ¥ SmartPremium: Predicting Insurance Costs with Machine Learning

![Python](https://img.shields.io/badge/Python-blue)
![Scikit-Learn](https://img.shields.io/badge/Scikit--Learn-orange)
![Streamlit](https://img.shields.io/badge/Streamlit-red)
![MLflow](https://img.shields.io/badge/MLflow-green)
![XGBoost](https://img.shields.io/badge/XGBoost-brightgreen)
![Random%20Forest](https://img.shields.io/badge/Random%20Forest-blueviolet)


A comprehensive machine learning project that predicts insurance premiums based on customer characteristics and policy details. This end-to-end solution includes data analysis, model training, MLflow experiment tracking, and a Streamlit web application for real-time predictions.

## ğŸ¯ Overview

Insurance companies use various factors such as **age**, **income**, **health status**, and **claim history** to estimate premiums for customers. This project builds a **machine learning model** that accurately predicts **insurance premiums** based on customer characteristics and policy details.

### Business Use Cases

- **ğŸ¢ Insurance Companies**: Optimize premium pricing based on risk factors
- **ğŸ’³ Financial Institutions**: Assess risk for loan approvals tied to insurance policies  
- **ğŸ¥ Healthcare Providers**: Estimate future healthcare costs for patients
- **ğŸ¤– Customer Service Optimization**: Provide real-time insurance quotes based on data-driven predictions

## âœ¨ Features

### ğŸ”§ Core Capabilities

- **ğŸ“Š Comprehensive EDA** with interactive visualizations
- **ğŸ”„ Automated Data Preprocessing** with missing value handling
- **ğŸ¯ Advanced Feature Engineering** with domain-specific features
- **ğŸ¤– Multiple ML Algorithms** (Random Forest, XGBoost) with comparison
- **ğŸ“ˆ MLflow Experiment Tracking** for model management
- **ğŸŒ Streamlit Web Application** for real-time predictions
- **ğŸ“ Model Persistence** with artifact storage

### ğŸ“Š Data Analysis Features

- Target variable distribution analysis
- Correlation matrix and feature relationships
- Numerical and categorical feature distributions
- Feature importance analysis
- Model performance visualization

### ğŸŒ Web Application Features

- Real-time premium predictions
- User-friendly input interface
- Feature importance visualization
- Risk factor analysis
- Responsive design with custom styling

## ğŸ“ Project Structure

```text
mini_project-4/
â”œâ”€ data/
â”‚  â”œâ”€ train.csv                    
â”‚  â”œâ”€ test.csv                     
â”‚  â””â”€ sample_submission.csv     
â”‚
â”œâ”€ main.py                         # Training pipeline with MLflow
â”œâ”€ streamlit_app.py                # Streamlit web application
â”‚
â”œâ”€ outputs/
â”‚  â”œâ”€ final_submission.csv         # Predictions
â”‚  â”œâ”€ model_artifacts.pkl          # Trained model & preprocessors
â”‚  â”œâ”€ target_distribution.png      # EDA visualizations
â”‚  â”œâ”€ numerical_distributions.png
â”‚  â”œâ”€ categorical_distributions.png
â”‚  â”œâ”€ correlation_matrix.png
â”‚  â””â”€ feature_importance.png
â”‚
â”œâ”€ requirements.txt                # Project dependencies
```

## ğŸ› ï¸ Installation

### Prerequisites

- Python 3.8 or higher
- pip (Python package manager)

### Step 1: Clone the Repository

```bash
git clone https://github.com/yourusername/mini_project-4.git
cd mini_project-4
```

### Step 2: Install the Dependencies
```bash
pip install -r requirements.txt
```

## Complete Workflow
### 1. Data Preparation
### Ensure data files are in project root:
```bash
# - train.csv
# - test.csv  
# - sample_submission.csv
```
### 2. Run Training Pipeline
```bash
python main.py
```
### 3. Monitor Experiments
```bash
mlflow ui
```
### 4. Deploy Web App
```bash
streamlit run streamlit_app.py
```

## Model Training Details
### The pipeline includes:
```bash 
1. Data Sampling: 10-20% of data for faster iteration

2. Feature Engineering: Interaction terms, demographic groups, risk flags

3. Model Comparison: Random Forest vs XGBoost with hyperparameters

4. Evaluation: RMSE, MAE, RÂ² metrics

5. Artifact Saving: Models, scalers, encoders persisted
```

## Evaluation Metrics
```bash
RMSE (Root Mean Squared Error): âˆš(Î£(yáµ¢ - Å·áµ¢)Â²/n)

MAE (Mean Absolute Error): Î£|yáµ¢ - Å·áµ¢|/n

RÂ² Score: Proportion of variance explained
```
## Feature Engineering
```bash
Interaction Features: AgeÃ—Health, IncomeÃ—Credit

Demographic Groups: Age categories, Income brackets

Risk Flags: Young driver, Senior citizen, Poor health

Encoding: Label encoding for categorical variables

```
